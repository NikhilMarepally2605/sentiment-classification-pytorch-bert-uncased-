{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.5.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers) (3.14.0)\nRequirement already satisfied: tokenizers==0.9.3 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nimport numpy as np\n\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":2,"outputs":[{"output_type":"stream","text":"cuda\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ndf.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment  \n0  I`d have responded, if I were going   neutral  \n1                             Sooo SAD  negative  \n2                          bullying me  negative  \n3                       leave me alone  negative  \n4                        Sons of ****,  negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.sentiment)\nplt.xlabel('sentiment')","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"Text(0.5, 0, 'sentiment')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_int(sentiment):\n    if sentiment == \"neutral\":\n        return 1\n    elif sentiment == \"negative\":\n        return 0\n    else:\n        return 2\n    \ndf[\"sentiment\"] = df.sentiment.apply(to_int)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text  sentiment  \n0  I`d have responded, if I were going          1  \n1                             Sooo SAD          0  \n2                          bullying me          0  \n3                       leave me alone          0  \n4                        Sons of ****,          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_MODEL_NAME = \"bert-base-cased\"","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5007b791d894ed5a9a027d4af914b0a"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking it on sample text\nsample_text = \"I wish I had enough time to spend with my family\"\n\ntokens = tokenizer.tokenize(sample_text)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_text}')\nprint(f'  Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","execution_count":9,"outputs":[{"output_type":"stream","text":" Sentence: I wish I had enough time to spend with my family\n  Tokens: ['I', 'wish', 'I', 'had', 'enough', 'time', 'to', 'spend', 'with', 'my', 'family']\nToken IDs: [146, 3683, 146, 1125, 1536, 1159, 1106, 4511, 1114, 1139, 1266]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#everthing can be done using encode plus\n\nencoding = tokenizer.encode_plus(\n            sample_text, \n            max_length=32,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n            )\n\nencoding.keys()","execution_count":10,"outputs":[{"output_type":"stream","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"dict_keys(['input_ids', 'attention_mask'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(encoding['input_ids'])","execution_count":11,"outputs":[{"output_type":"stream","text":"tensor([[ 101,  146, 3683,  146, 1125, 1536, 1159, 1106, 4511, 1114, 1139, 1266,\n          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(encoding['attention_mask'])","execution_count":12,"outputs":[{"output_type":"stream","text":"tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text  sentiment  \n0  I`d have responded, if I were going          1  \n1                             Sooo SAD          0  \n2                          bullying me          0  \n3                       leave me alone          0  \n4                        Sons of ****,          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['selected_text'].isna().sum()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_lens = []\nfor txt in df.selected_text:\n    tokens = tokenizer.encode(txt, max_length=512)\n    token_lens.append(len(tokens))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count')","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"Text(0.5, 0, 'Token count')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 60","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Sentiment(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        \n        encoding = self.tokenizer.encode_plus(\n            review, \n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'review_text':review,\n            'input_ids':encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(target, dtype=torch.long)\n        }","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df, test_size=0.1,random_state=RANDOM_SEED)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"(24732, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"(2748, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text  sentiment  \n0  I`d have responded, if I were going          1  \n1                             Sooo SAD          0  \n2                          bullying me          0  \n3                       leave me alone          0  \n4                        Sons of ****,          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    \n    ds = Sentiment(\n    reviews = df[\"selected_text\"].to_numpy(),\n    targets = df['sentiment'].to_numpy(),\n    tokenizer = tokenizer,\n    max_len = max_len\n    )\n    \n    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = next(iter(train_data_loader))","execution_count":26,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","execution_count":27,"outputs":[{"output_type":"stream","text":"torch.Size([16, 60])\ntorch.Size([16, 60])\ntorch.Size([16])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n\nlast_hidden_state, pooled_output = bert_model(\n  input_ids=encoding['input_ids'],\n  attention_mask=encoding['attention_mask']\n)","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462eef6760f549adb25f0dad63dd6b41"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b91bde3a70d748ce8a75f144818a9af9"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_hidden_state.shape","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"torch.Size([1, 32, 768])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model.config.hidden_size","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"768"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pooled_output.shape","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"torch.Size([1, 768])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n  \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        output = self.drop(pooled_output)\n        return self.out(output)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names_len = len(df['sentiment'].unique())\n\nmodel = SentimentClassifier(class_names_len)\nmodel = model.to(device)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"SentimentClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.3, inplace=False)\n  (out): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch( model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            \n            outputs = model(input_ids = input_ids, attention_mask=attention_mask)\n            \n            _, preds = torch.max(outputs, 1)\n            \n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n            \n    \n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    len(df_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n    model,\n    val_data_loader,\n    loss_fn,\n    device,\n    len(df_test)\n    )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","execution_count":39,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.2243004103718605 accuracy 0.9312227074235808\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.38900990013931985 accuracy 0.8948326055312955\n\nEpoch 2/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.14404844494044353 accuracy 0.9627203622836811\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.48322226497382575 accuracy 0.8941048034934498\n\nEpoch 3/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.09374516769208406 accuracy 0.9771955361475012\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.6467420109050203 accuracy 0.8828238719068414\n\nEpoch 4/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.06425448313953637 accuracy 0.9857674268154618\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.6797963328193196 accuracy 0.886098981077147\n\nEpoch 5/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.04470410266728698 accuracy 0.990336406275271\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.7758106688397252 accuracy 0.883551673944687\n\nEpoch 6/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.03504465489565941 accuracy 0.9923176451560731\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.8355017901268966 accuracy 0.8850072780203785\n\nEpoch 7/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.028456968579949476 accuracy 0.9943393174834223\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.8806017500848907 accuracy 0.88136826783115\n\nEpoch 8/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.01866987391007273 accuracy 0.9960375222383956\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.9314959601961587 accuracy 0.8820960698689957\n\nEpoch 9/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.014670446723059926 accuracy 0.9967248908296944\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.951436831997007 accuracy 0.8817321688500728\n\nEpoch 10/10\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Train loss 0.013809613092053747 accuracy 0.9966035904900534\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"Val   loss 0.951436831997007 accuracy 0.8817321688500728\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\nsample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"       textID                                               text sentiment\n0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n3  01082688c6                                        happy bday!  positive\n4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f87dea47db</td>\n      <td>Last session of the day  http://twitpic.com/67ezh</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>96d74cb729</td>\n      <td>Shanghai is also really exciting (precisely -...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eee518ae67</td>\n      <td>Recession hit Veronique Branquinho, she has to...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01082688c6</td>\n      <td>happy bday!</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33987a8ee5</td>\n      <td>http://twitpic.com/4w75p - I like it!!</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"       textID  selected_text\n0  f87dea47db            NaN\n1  96d74cb729            NaN\n2  eee518ae67            NaN\n3  01082688c6            NaN\n4  33987a8ee5            NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>selected_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f87dea47db</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>96d74cb729</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eee518ae67</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01082688c6</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33987a8ee5</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":47,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'test_data_loader' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-5fadc581fb13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreview_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test_data_loader' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}